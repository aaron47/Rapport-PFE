\chapter{Étude et réalisation du Sprint 1}
\etocsettocstyle{\section*{Sommaire}}{}
\localtableofcontents
\section{Introduction}
\noindent
Après avoir examiné l'étude technique de notre projet d'amélioration de recherche et intégration de recherche intelligente, nous entamons maintenant le Sprint 1, qui ce concentra sur la création d'un systéme de recherche des produits dans la langue Française.

\section{Backlog du Sprint 1}
\noindent
Le backlog de notre premier Sprint est présenté dans le tableau ~\ref{tab:sprint1}.

\begin{table}[H]
	\centering

	\begin{tabularx}{\textwidth}{|c|X|c|c|}
		\hline
		\rowcolor{blue!20}
		\textbf{ID} & \textbf{Scénario}                                                                                                                                                      & \textbf{Priorité} & \textbf{Complexité} \\ \hline
		1           & En tant qu'un client, visiteur, je veux saisir mon terme de recherche en français pour chercher le(s) produit(s)                                                        & 1                 & 10                  \\ \hline
		2           & En tant qu'un client, visiteur, si le produit exacte que je cherche n'existe pas, je veux voir des suggestions de produits similaires. & 2                 & 8                   \\ \hline
	\end{tabularx}
	\caption{Backlog du Sprint 1}
	\label{tab:sprint1}
\end{table}

\section{Spécification fonctionnelle}
\noindent
Dans cette partie, on va expliquer les différentes fonctionnelités du Sprint 1 à travers le diagramme de cas d'utilisation. Puis on va exposer les différents scénarios de notre cas d'utilisation à travers des descriptions textuelles.

\subsection{Diagramme de cas d'utilisation générale}
\noindent
La figure ~\ref{fig:recherchefrancais} illustre le diagramme de cas d'utilisation gènèrale de notre premier sprint, qui est la recherche en français.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{logos/cusprint1.png}
	\caption{Diagramme de cas d'utilisation générale du Sprint 1}
	\label{fig:recherchefrancais}
\end{figure}

\subsection{Description textuelle du CU << Rechercher produit en français >>}
\noindent
\textbf{Titre:} Rechercher produit en français \\
\textbf{Résumé:} Le client saisit son terme de recherche (en français), en cliquant sur la boutton pour rechercher le(s) produit(s) qu'il veut chercher. \\
\textbf{Acteur Principal:} Client, Visiteur \\
\textbf{Précondition:} \begin{enumerate}
	\item Le client (ou le visiteur) est sur la page de recherche
	\item Le client (ou le visiteur) a saisi son terme de recherche
	\item Le client (ou le visiteur) a cliqué sur "Rechercher"
\end{enumerate}
\textbf{Postcondition:} Le(s) produit(s) que le client cherche est renvoyé, s'il n'existe pas, le systéme renvoie des produits similaires comme suggestion. \\
\textbf{Scénario de base: }
\begin{enumerate}
	\item Le client saisit son terme de recherche.
	\item Le client clique sur le boutton "Rechercher"
	\item Le système prend ce terme de recherche, et performe les étapes nécessaires pour la convertir en vecteur.
	\item Le système compare ce vecteur contre les vecteurs dans Elasticsearch.
	\item Le système renvoie les produits.
\end{enumerate}

\textbf{Scénario alternatifs : }
\begin{enumerate}
	\item Le terme de recherche est vide:
	      \begin{enumerate}
		      \item Le système affiche un message d'erreur informant le client que le terme de recherche est requis.
		      \item Retour à l'étape 1 du scénario de base.
	      \end{enumerate}

	\item Le(s) produit(s) que le client cherche n'existe pas.
	      \begin{enumerate}
		      \item Le système essaie de renvoyer les produits les plus similaires comme des suggestions.
		      \item Retour à l'étape 1 du scénario de base.
	      \end{enumerate}
\end{enumerate}

\subsection{Description textuelle du CU << Voir suggestions >>}
\noindent
\textbf{Titre:} Voir suggestions \\
\textbf{Résumé:} Après que le client a cherché un produit, le système essaie de lui suggérer des produits similaires. \\
\textbf{Acteur Principal:} Client \\
\textbf{Précondition:} \begin{enumerate}
	\item Le client est authentifié.
	\item Le client a déja cherché un produit.
\end{enumerate}
\textbf{Postcondition:} Le(s) produit(s) que le client cherche est renvoyé, et le systéme renvoie des produits similaires comme suggestions. \\
\textbf{Scénario de base: }
\begin{enumerate}
	\item Le client cherche un produit.
	\item Le système cherche le produit.
	\item Le système renvoie les produits ainsi que des suggestions des produits similaires.
\end{enumerate}

\newpage
\textbf{Scénario alternatifs : }
\begin{enumerate}
	\item Le(s) produit(s) que le client cherche n'existe pas.
	      \begin{enumerate}
		      \item Le système essaie de renvoyer les produits les plus similaires comme des suggestions.
		      \item Retour à l'étape 1 du scénario de base.
	      \end{enumerate}
\end{enumerate}

\subsection{Diagramme de séquence detaillée}
\noindent
En adoptant l'architecture MVC dans le chapitre précédent, nous avons choisi de
suivre ce modèle pour simplifier la création des diagrammes de séquence. Cette section
présentera le diagramme de séquence de cas d'utilisation << Rechercher produits en français >> qui est présenté dans la figure ~\ref{fig:seqrecherchefrancais}.

% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=1\textwidth]{logos/seqsprint1.png}
% 	\caption{Diagramme de séquence de cas d’utilisation « Rechercher produits en Français »}
% 	\label{fig:seqrecherchefrancais}
% \end{figure}

\afterpage{
	\newgeometry{margin=1in, paperwidth=420cm, paperheight=297mm}
	\begin{figure}[p]
		\includegraphics[width=\textwidth,height=200mm]{logos/seqsprint1.png}
		\caption{Diagramme de séquence de cas d’utilisation « Rechercher produits en français »}
		\label{fig:seqrecherchefrancais}
	\end{figure}
	\clearpage
	\restoregeometry
}

\newpage
\section{Architecture de la base de données}
\noindent
Dans le cadre de notre projet, l'intégration d'Elasticsearch et MySQL génère un ensemble de tables indispnesables pour garantir son fonctionnement interne. Cependant, nous avons décidé de mettre l'accent uniquement sur les tables
spécifiques à notre projet durant la conception.

\subsection{Diagramme de classes}
\noindent
Les diagrammes de classes sont l'un des types de diagrammes UML les plus utiles, car ils décrivent clairement la structure d’un système particulier en modélisant ses classes, ses attributs, ses opérations et les relations entre ses objets. \citetitle{lucidcharts:classdiagram} (\cite{lucidcharts:classdiagram}). \\
La figure ~\ref{fig:diagclasses} montre la conception UML diagramme de classe général de notre projet.

\afterpage{
	\newgeometry{margin=1in, paperwidth=420mm, paperheight=297mm}
	\begin{figure}[p]
		\includegraphics[width=\textwidth,height=500mm]{logos/classes.png}
		\caption{Diagramme de classes globale}
		\label{fig:diagclasses}
	\end{figure}
	\clearpage
	\restoregeometry
}

\newpage
\subsection{Schéma de la base de données}
\noindent
Suite à l'exploration du modèle conceptuel de la base de données pour le premier
sprint, nous allons maintenant décrire sa transposition en modèle logique, illustré par les tables dans les sections suivantes.

\subsection{Tables de base de données MySQL}
\begin{table}[H]
	\centering
	\Large
	\rowcolors{2}{white}{white} % To reset alternate colors if previously set
	\begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|}
		\hline
		\rowcolor{blue!50} \textcolor{white}{Champs} & \textcolor{white}{Type} & \textcolor{white}{Contrainte} \\
		\hline
		id                                           & Auto-incrément          & Clé primaire                  \\ \hline
		email                                        & String                  & Non nul                       \\ \hline
		password                                     & String                  & Non nul                       \\ \hline
		role                                         & String                  & Non nul                       \\ \hline
	\end{tabular}
	\caption{Table Client}
	\label{tab:table_client}
\end{table}

\begin{table}[H]
	\centering
	\Large
	\rowcolors{2}{white}{white} % To reset alternate colors if previously set
	\begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|}
		\hline
		\rowcolor{blue!50} \textcolor{white}{Champs} & \textcolor{white}{Type} & \textcolor{white}{Contrainte}                   \\
		\hline
		id                                           & Auto-incrément          & Clé primaire                                    \\ \hline
		word                                         & String                  & Non nul                                         \\ \hline
		count                                        & Int                     & Non nul                                         \\ \hline
		recherche\_id                                 & Int                     & Clé étrangère  \\ \hline
	\end{tabular}
	\caption{Table Keyword}
	\label{tab:table_keyword}
\end{table}

\begin{table}[H]
\centering
\Large
\rowcolors{2}{white}{white} % To reset alternate colors if previously set
\begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|}
\hline
\rowcolor{blue!50} \textcolor{white}{Champs} & \textcolor{white}{Type} & \textcolor{white}{Contrainte} \\
\hline
id & Auto-incrément & Clé primaire \\ \hline
title & String & Non null \\ \hline
product\_id & Int & Unique,\\ Clé étrangère \\ \hline
\end{tabular}
\caption{Table Tags}
\label{tab:table_tags}
\end{table}

\begin{table}[H]
\centering
\Large
\rowcolors{2}{white}{white} % To reset alternate colors if previously set
\begin{tabular}{|p{4cm}|p{4cm}|p{4cm}|}
\hline
\rowcolor{blue!50} \textcolor{white}{Champs} & \textcolor{white}{Type} & \textcolor{white}{Contrainte} \\
\hline
id & Auto-incrément & Clé primaire \\ \hline
date & DateTime & Non null \\ \hline
\end{tabular}
\caption{Table Recherche}
\label{tab:table_recherche}
\end{table}

\subsection{Tables de base de données Elasticsearch}
\begin{longtable}{|l|l|p{5cm}|}
	\caption{Table Produit} \label{tab:produit_table}                                                                   \\
	\hline
	\rowcolor{blue!50} \textcolor{white}{Champs} & \textcolor{white}{Type} & \textcolor{white}{Contrainte}              \\
	\hline
	\endfirsthead

	\multicolumn{3}{c}%
	{{\bfseries \tablename\ \thetable{} -- Suivie de la page précédente}}                                               \\
	\hline
	\rowcolor{blue!50} \textcolor{white}{Champs} & \textcolor{white}{Type} & \textcolor{white}{Contrainte}              \\
	\hline
	\endhead

	% \hline \multicolumn{3}{|r|}{{Suivie sur la page suivante}} \\ \hline
	% \endfoot

	\hline
	code interne                                 & keyword                 &                                            \\ \hline
	image produit                                & keyword                 &                                            \\ \hline
	code a barre                                 & keyword                 &                                            \\  \hline
	REFERENCE                                    & keyword                 &                                            \\ \hline
	SKU                                          & keyword                 &                                            \\ \hline
	label produit                                & text                    &                                            \\ \hline
	SEO label produit                            & text                    &                                            \\ \hline
	categorie                                    & keyword                 &                                            \\ \hline
	sous-categorie                               & keyword                 &                                            \\ \hline
	sous-sous-categorie                          & keyword                 &                                            \\ \hline
	categorie\_id                                & integer                 &                                            \\ \hline
	collection                                   & text                    &                                            \\ \hline
	Brève description                            & text                    &                                            \\ \hline
	Description                                  & text                    &                                            \\ \hline
	Tags                                         & text                    &                                            \\ \hline
	fiche technique                              & text                    &                                            \\ \hline
	alt image(71 caracteres)                     & text                    &                                            \\ \hline
	link                                         & keyword                 &                                            \\ \hline
	meta-description                             & text                    &                                            \\ \hline
	meta title                                   & text                    &                                            \\ \hline
	old\_optimization grade                      & keyword                 &                                            \\ \hline
	new\_optimization grade                      & keyword                 &                                            \\ \hline
	Poids                                        & float                   &                                            \\ \hline
	Couleur                                      & keyword                 &                                            \\ \hline
	color\_id                                    & integer                 &                                            \\
	\hline
	Marque                                       & keyword                 &                                            \\ \hline
	marque\_id                                   & integer                 &                                            \\ \hline
	garantie                                     & keyword                 &                                            \\ \hline
	Stock                                        & float                   &                                            \\ \hline
	fabriqué en                                  & keyword                 &                                            \\ \hline
	est retournable                              & keyword                 &                                            \\ \hline
	Prix vendeur                                 & float                   &                                            \\ \hline
	Prix brute                                   & float                   &                                            \\ \hline
	Prix Promo                                   & float                   &                                            \\ \hline
	lien (web et video)                          & keyword                 &                                            \\ \hline
	lien                                         & keyword                 &                                            \\ \hline
	image principale                             & keyword                 &                                            \\ \hline
	images secondaires                           & keyword                 &                                            \\ \hline
	seller-id                                    & keyword                 &                                            \\ \hline
	Created by                                   & text                    &                                            \\ \hline
	LabelProduitVecteur                          & dense\_vector           & dims: 768, index: true, similarity: cosine \\ \hline
	DescriptionVecteur                           & dense\_vector           & dims: 768, index: true, similarity: cosine \\
	\hline
\end{longtable}

\section{Réalisation}
\noindent
Cette partie est consacrée à la présentation de l'interface de recherche pour le client et à approfondir les détails de fonctionnement de recherche et Elasticsearch.

\subsection{La création des colonnes des vecteurs}
\noindent
D'abord on a commencé par la création de notre index (table) de produit pour Elasticsearch et insérer les données là. La figure ~\ref{fig:indexmappingproduit} montre le code nécessaire pour créer cet index.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,height=0.75\textheight,keepaspectratio]{logos/index_mapping.png}
	\caption{Code d'index de Produit}
	\label{fig:indexmappingproduit}
\end{figure}

\newpage
\noindent
On commence par la création d'un objet << index\_mapping >> qui contient un objet << propriétés >> contenant toutes les colonnes de l'index dans Elasticsearch, le reste des colonnes sont les mêmes dans le tableau ~\ref{tab:produit_table}. Concentrons-nous sur les 2 dernières colonnes, qui sont les plus importantes, les vecteurs que nous allons utiliser pour notre recherche. Les deux colonnes sont de type dense\_vector, indiquant qu'elles sont des vecteurs, la contrainte << dims >> qui a une valeur de 768, indique que ces vecteurs sont à 768 dimensions, << index >> qui a la valeur True, indique qu'ils sont indexables, c'est à dire qu'on peut utiliser cette colonne pour effectuer une comparaison du similarité, qui est de type << cosine >> qui indique que Elasticsearch va effectuer une similarité cosinus, qu'on va expliquer avec plus de détails dans les sections suivantes.

\subsection{Le modèle Sentence-Transformers et encodage des phrases}
\noindent
Comme on a mentionné, le modèle Sentence-Transformers qu'on va utiliser est << all-mpnet-base-v2 >>, qu'on l'importe de cette façon:

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth,height=0.75\textheight,keepaspectratio]{logos/import_model.png}
	\caption{Code d'importation du modèle Sentence-Transformers}
	\label{fig:importmodel}
\end{figure}

\newpage
\noindent
On utilise AutoTokenizer pour importer le Tokeniser du modèle, aussi qu'AutoModel pour importer le modèle. Le modèle a besoin d'un << Tokenizer >> puisqu'il ne peut pas comprendre du texte, on doit convertir chaque phrase en une représentation numérique. Le processus est appelé la << tokenisation >> qui est le processus de conversion d'une séquence de caractères en une séquence de jetons(tokens), ce token représente généralement un mot, il y'a aussi des tokens comme le token << CLS >> qui est le << Classify Token >>, il est mis au début, pour marquer que c'est une phrase, et le token << PAD >>, qui es utilisé quand il y a plusieurs phrases à tokeniser, et pour ça, il a besoin de rendre toutes les phrases de même longueur. Le token << CLS >> a un identifiant de 101, et << PAD >> a un identifiant de 0. La figure ~\ref{fig:tokenisationexp} illustre un exemple simple de Tokenisation.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{logos/tokenisation.png}
	\caption{Exemple de processus de Tokenisation}
	\label{fig:tokenisationexp}
\end{figure}

\subsubsection{Comment le modèle encode une phrase?}
\noindent
On a créé une méthode appelée "encode\_sentence\_and\_normalise" pour faire l'encodage en faisant les étapes mentionnée dans la partie précédente en ajoutant une autre étape qui est trés importante, qui est le Mean Pooling. \\
D'abord, on utilise le tokeniser pour effectuer la tokenisation et le padding sur la phrase, on a effectué << truncation >> à True au cas où la phrase dépasse la limite des mots par phrase pour notre modèle qui est 368 mots, il va seulement prendre les 368 premiers mots si la phrase dépasse la limite. Ensuite, on utilise la méthode << no\_grad >> de Pytorch, pour désactiver les << Gradients >> et passer les séquences tokenisées au modèle pour faire l'encodage.

\subsubsection{Que'est ce qu'un << Gradient >>?}
\noindent
Un gradient consiste à mettre à jour les poids de chaque neurone de la dernière couche vers la première. Il vise à corriger les erreurs selon l'importance de la contribution de chaque élément à celles-ci. Mais dans notre cas, on désactive les calculs des << gradients >> pour un nombre de raisons importantes qui sont citèes dans \citetitle{pytorch:nograd} (\cite{pytorch:nograd}), tels que:

\begin{enumerate}
	\item \small\textbf{Contrôle du calcul du gradient: }Dans notre cas, le modèle est utilisé pour l'inférence, c'est-à-dire pour générer des vecteurs pour une phrase d'entrée donnée. Puisqu'il n'est pas nécessaire de calculer les gradients pendant l'inférence, l'utilisation de torch.no\_grad() évite une consommation inutile de mémoire et une surcharge de calcul en désactivant le suivi des gradients.
	\item \small\textbf{Optimisation de la mémoire: }Lors de l'inférence, il n'est pas nécessaire de calculer les gradients car les paramètres du modèle ne sont pas mis à jour. En désactivant le calcul du gradient, nous économisons de la mémoire qui serait autrement utilisée pour stocker les informations sur le dégradé. Cela peut être particulièrement important pour les grands modèles ou lorsqu’il s’agit de longues séquences.

	\item \small\textbf{Optimisation de la vitesse: }La désactivation du calcul du gradient accélère également le processus, car le framework n'a pas besoin d'effectuer les calculs supplémentaires requis pour le suivi du gradient.
\end{enumerate}

\noindent
Après avoir faire l'encodage de phrase, le modèle nous donne un output qui consiste de plusieurs vecteurs qui représentent chaque mot de la phrase. De coup, on a plusieurs vecteurs, c'est à dire chaque mot est isolé, donc on a besoin d'une méthode pour regrouper ces mots, et prendre le contexte de toute la phrase, c'est là qu'intervient la méthode de << Mean Pooling >>.

\subsubsection{Le Mean Pooling}
\noindent
Le Mean Pooling est un processus qui calcule efficacement la moyenne de la sequence obtenue après le padding et la tokenisation tout en ignorant les jetons de remplissage (padding tokens) qui ont une valeur de 0, ce qui donne un seul vecteur qui représente la phrase entière. La figure ~\ref{fig:meanpoolingex} illustre un exemple.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{logos/mean_pooling.png}
	\caption{Exemple de Mean Pooling}
	\label{fig:meanpoolingex}
\end{figure}

\noindent
Nous prenons la moyenne de chaque vecteur et on le met dans un seul vecteur. La figure ~\ref{fig:meanpoolingmethod} présente le code nécessaire pour cette methode.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{logos/mean_pooling_method.png}
	\caption{Code de méthode mean\_pooling}
	\label{fig:meanpoolingmethod}
\end{figure}

\noindent
Cette méthode consiste de trois étapes, qui sont:
\begin{enumerate}
	\item \small\textbf{L'éxtraction des vecteurs des tokens: }token\_embeddings = model\_output[0] : cette ligne récupère les vecteurs de tokens à partir de l'output du modèle. Généralement, pour les modèles de Sentence-Transformers, le premier élément de la sortie (model\_output[0]) contient les intégrations de tous les tokens de la séquence d'entrée.

	\item \small\textbf{Extension du masque d'attention: }input\_mask\_expanded = \\ attention\_mask.unsqueeze(-1).expand(token\_embeddings.size()).float() : \\ Cette ligne traite le attention\_mask. Le masque d'attention est simplement un masque qui fait la différence entre le contenu et les jetons de remplissage.

	      \begin{itemize}
		      \item unsqueeze(-1) ajoute une dimension supplémentaire à la fin du attention\_mask, le rendant compatible en dimensions avec token\_embeddings lorsque nous appliquons expand().
		      \item expand(token\_embeddings.size()) ajuste le masque pour qu'il corresponde aux dimensions de token\_embeddings, répétant efficacement le masque pour chaque dimension vecteur.
		      \item .float() convertit le masque en float, facilitant les opérations mathématiques ultérieures avec token\_embeddings.
	      \end{itemize}

	\item \small\textbf{Application du masque et calcul de Mean Pooling: }
	      \begin{itemize}
		      \item torch.sum(token\_embeddings * input\_mask\_expanded, 1) : ceci calcule la somme des vecteurs dans la dimension de séquence (dimension 1), mais uniquement pour les vecteurs correspondant aux jetons de données réelles (pas de padding(0)), comme indiqué par input\_mask\_expanded.
		      \item torch.clamp(input\_mask\_expanded.sum(1), min=1e-9) : La somme des vecteurs est ensuite divisée par la somme de input\_mask\_expanded le long de la dimension de séquence, ce qui donne le nombre de tokens sans padding. torch.clamp garantit que nous ne divisons pas par zéro en définissant une valeur minimale (1e-9), empêchant ainsi les erreurs de division par zéro.
	      \end{itemize}
\end{enumerate}

\noindent
Après l'étape de Mean Pooling, on obtient un vecteur qui représente la phrase, que l'on ensuite normalise avec la fonction normalize de Pytorch.
La figure ~\ref{fig:encodesentence} illustre la méthode compléte pour l'encodage d'une phrase.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{logos/encode_sentence.png}
	\caption{Méthode encode\_sentence\_and\_normalise}
	\label{fig:encodesentence}
\end{figure}

\subsection{Préparation des données dans Elasticsearch pour recherche vectorielle}
\noindent
Pour effectuer notre méthode de recherche qui est la recherche vectorielle, il faut d'abord ajouter les vecteurs avec lesquels nous voulons comparer, et les ajouter dans notre base de données qui est Elasticsearch. Pour effectuer ça, on va utiliser la méthode qu'on a crée << encode\_sentence\_and\_normalise >> pour générer les vecteurs, mais d'abord, on doit créer une instance de notre modèle, on a appelé cette classe AllMpnetBaseV2. Dans notre cas on veut utiliser les colonnes << Bréve Description >> et << SEO Label Produit >>, donc on va faire l'encodage pour chaque ligne dans deux nouvelles colonnes << DescriptionVecteur >> et << LabelProduitVecteur >>. \\ La figure ~\ref{fig:encoding} montre le code pour cette étape.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{logos/vectors.png}
	\caption{Encodage des deux colonnes Brève Déscription et SEO Label Produit}
	\label{fig:encoding}
\end{figure}

\noindent
Le résultat de cette étape c'est qu'on obtient 2 nouvelles colonnes, << DescriptionVecteur >> et << LabelProduitVecteur >> qui sont montrées dans la figure ~\ref{fig:encodedvectors}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{logos/encodedvectors.png}
	\caption{les deux nouvelles colonnes << descriptionvecteur >> et << labelproduitvecteur >> }
	\label{fig:encodedvectors}
\end{figure}

\noindent
L'étape suivante consiste de faire une connexion à Elasticsearch, et insérer les données des produits. D'abord on établit une connexion à notre cluster Elasticsearch qu'on a lancé à partir de Docker, en créant une instance de class Elasticsearch et spécifiant le host, et le basic auth qui consiste de nom utilisateur et mot de passe généré par Kibana. Ensuite, nous créons notre index Elasticsearch en spécifiant notre index\_mapping qu'on a mentionné au debut de ce chapitre à travers la méthode << es.indices.create >> qui prend deux paramètres, le nom de l'index et son mapping. Enfin, nous convertissons notre CSV en object Python à travers la méthode << to\_dict >> et nous insérons chaque ligne dans Elasticsearch à travers la méthode << index >>, qui prend trois paramètres << index >>, << document >> et << id >>. \\
La figure ~\ref{fig:insertintoelastic} montre le code nécessaire pour cette étape.

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{logos/insert_into_elastic.png}
	\caption{Insértion des données dans Elasticsearch}
	\label{fig:insertintoelastic}
\end{figure}

\subsection{Elasticsearch et Similarité Cosinus}
\noindent
Après avoir préparé notre données dans Elasticsearch, on ajoute les colonnes des vecteurs qu'on va comparer contre, on peut maintenant performer la recherche vectorielle en utilisant Elasticsearch. \\
Quand un client saisi son terme de recherche à travers notre interface, il est envoyé à notre API ASP .NET Core, qui ensuite l'envoie à notre API Flask qui contient notre modèle Sentence-Transformers, pour effectuer les étapes nécessaires pour l'encodage de phrase en vecteur, et ensuite le renvoyer au contrôleur pour l'utiliser dans un Elasticsearch query knn, pour ça on a créé la méthode << KnnSearchAsync >>, qui est montré dans la figure ~\ref{fig:knn}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{logos/knnsearch.png}
	\caption{Méthode KnnSearchAsync}
	\label{fig:knn}
\end{figure}

\noindent
Cette méthode prend 2 paramètres qui sont:
\begin{itemize}
	\item \small\textbf{queryVector: } Qui est une liste des réels représentant le vecteur encodé.
	\item \textbf{knnSearchRequest: } Qui est une classe qui prend 4 attributs utilisé dans note query knn:
	      \begin{enumerate}
		      \item \small\textbf{TopResProdLabel: } Un entier représentant le nombre des meilleurs résultats par recherche vectorielle de << LabelProduit >>, valeur par défaut: 5.
		      \item \small\textbf{TopResDesc: } Un entier représentant le nombre des meilleurs résultats par recherche vectorielle de << Description >>, valeur par défaut: 5.
		      \item \small\textbf{NumCandidatesDesc: } Un entier représentant le nombre total des produits que nous souhaitons rechercher par la description, valeur par défaut: 20.
		      \item \small\textbf{NumCandidatesProdLabel: } Un entier représentant le nombre total des produits que nous souhaitons rechercher par le label produit, valeur par défaut: 10.
	      \end{enumerate}

\end{itemize}

\noindent
Cette recherche trouve les principales correspondances vectorielles globales k = 25 pour le vecteur << LabelProduit >> et la valeur globale k = 25 aussi pour le vecteur << Description >>. Ces valeurs principales sont ensuite combinées avec les correspondances de la requête de correspondance et les 10 premiers documents sont renvoyés si k > 10, sinon le nombre de résultats renvoyés sont calculé comme ça:\[
	\begin{aligned}
		 & kTopResProdLabel + kTopResDescription                                \\
		 & \text{Si } kTopResProdLabel < 10 \text{ Et } kTopResDescription < 10 \\
	\end{aligned}
\]
Les multiples entrées knn et les correspondances de requêtes sont combinées via une disjonction, comme si vous preniez un booléen ou entre elles. Les k premiers résultats vectoriels représentent les voisins globaux les plus proches sur toutes les partitions d'index.
\noindent
La notation d'un document avec les améliorations configurées ci-dessus serait comme le suivant, notons:
\begin{itemize}
	\item \( \mathbf{q} \) comme vecteur de requête.
	\item \( \mathbf{v}_{\text{label}} \) and \( \mathbf{v}_{\text{desc}} \) comme vecteurs de document dans les champs ``LabelProduitVecteur'' et ``DescriptionVecteur'', respectivement.
	\item \( \text{score}_{\text{label}} \) and \( \text{score}_{\text{desc}} \) comme les scores basés sur ces vecteurs.
\end{itemize}

\noindent
En utilisant la similarité cosinus, le score de chaque composant peut être calculé comme suit :
\[
	\text{score}_{\text{label}}(\mathbf{q}, \mathbf{v}_{\text{label}}) = \frac{\mathbf{q} \cdot \mathbf{v}_{\text{label}}}{\|\mathbf{q}\| \|\mathbf{v}_{\text{label}}\|} = \frac{\sum_{i=1}^n q_i v_{\text{label}, i}}{\sqrt{\sum_{i=1}^n q_i^2} \sqrt{\sum_{i=1}^n v_{\text{label}, i}^2}}
\]
\[
	\text{score}_{\text{desc}}(\mathbf{q}, \mathbf{v}_{\text{desc}}) = \frac{\mathbf{q} \cdot \mathbf{v}_{\text{desc}}}{\|\mathbf{q}\| \|\mathbf{v}_{\text{desc}}\|} = \frac{\sum_{i=1}^n q_i v_{\text{desc}, i}}{\sqrt{\sum_{i=1}^n q_i^2} \sqrt{\sum_{i=1}^n v_{\text{desc}, i}^2}}
\]
\[
	\text{Score Combinée} = \alpha \cdot \text{score}_{\text{label}} + \beta \cdot \text{score}_{\text{desc}}
\]

\noindent
Ici:
\begin{itemize}
	\item \( \cdot \) représente le produit scalaire.
	\item \( \|\mathbf{q}\| \) and \( \|\mathbf{v}\| \) désignent les normes (grandeurs) des vecteurs, qui sont calculés comme la racine carrée de la somme des composantes au carré des vecteurs.
	\item \( \alpha \) and \( \beta \) sont des pondérations qui pourraient être utilisées pour équilibrer l'importance des scores des deux domaines (en supposant une importance égale, les deux pourraient être fixées à 0,5 par défaut, ou ajustées en fonction de besoins spécifiques).
\end{itemize}

\noindent
La figure ~\ref{fig:vectorsearch} illustre le processus complet de recherche.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{logos/vectorsearch.png}
	\caption{Processus de recherche véctorielle Elasticsearch}
	\label{fig:vectorsearch}
\end{figure}

\noindent
L'implementation de ce processus se produit dans la fonction suivante:

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{logos/returnencodedvector.png}
	\caption{Encodage de terme de recherche et recherche vectorielle dans Elasticsearch}
	\label{fig:returnencodedvector}
\end{figure}


\noindent
La figure ~\ref{fig:testsearch} illustre la recherche des produits à partir de Postman.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{logos/testsearch.png}
	\caption{Recherche à partir de Postman}
	\label{fig:testsearch}
\end{figure}


\noindent
La figure ~\ref{fig:testsearch2} illustre l'interface de recherche de client, ou il a cherché pour << lambaa taa tawla >> qui se traduit en << lampe de table >>, avec des produits renvoyés ainsi que des suggestions des produits potentiellement similaires.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{logos/testsearch2.png}
	\caption{Interface de recherche de client}
	\label{fig:testsearch2}
\end{figure}

\newpage
\section{Conclusion}
\noindent
Dans ce chapitre, nous avons couvert toutes les étapes nécessaires pour performer la recherche vectorielle dans notre projet, en commençant par la création de la fonction d'encodage << encode\_sentence\_and\_normalise >>à l'aide de notre modèle et de son tokeniser, ensuite la préparation de nos données pour ensuite insérez-les dans Elasticsearch pour avoir la capabilité de recherche vectorielle. Enfin nous préparons notre contrôleur ASP .NET Core pour atteindre l'endpoint << /encode >> afin d'encoder le mot-clé de recherche saisi par l'utilisateur, et l'utiliser dans une query knn à l'aide de la méthode << KnnSearchAsync >> pour performer une query Elasticsearch knn pour renvoyer des produits et des suggestions.
Grâce à ces étapes, le travail pour le reste des sprints a été rendu beaucoup plus facile et évolutif.